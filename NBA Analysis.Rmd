---
title: "NBA Data Analysis"
output: html_document
date: "2024-04-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background
The data consists of all NBA final score and boxscore at half time for 2023-24 regular season. The goal is to use boxscore information at halftime to predict the final score. The data Source is nba_api (https://github.com/swar/nba_api), and the data were pulled and cleaned separately using Python. 

### Attributes:
- GAME_ID: id of the game

- team: the boxscore information of the team

- against: the opposing team

- s_: all information for starters

- b_: all information for bench players

- minutes: minutes played

- PTS_final: final score by team at end of game

- home_away: team is at home or away

- PTS_ht: team score at halftime

- PTS_ht_opp: opponent score at halftime

### Set up environment

```{r load data, echo=FALSE}
library(olsrr)
library(stats)
library(leaps)
library(MASS)
library(glmnet)
library(corrplot)
library(car)
data = read.csv('NBA_2023_halftime_boxscore_data_for_analysis.csv')
head(data)
attach(data)

```

### Clean and reformat data
```{r reformatting data}

# Add s_2PA, b_2PM, and b_2PA and remove s_FGM, s_FGA, b_FGM, and b_FGA since FG includes both 2 pointers and 3 pointers
# s_2PM is not added because it'll cause multicolliearity
data$s_2PA <- data$s_FGA - data$s_3PA
data$b_2PM <- data$b_FGM - data$b_3PM
data$b_2PA <- data$b_FGA - data$b_3PA

# Create dummy variable, home, for home_away, and drop home_away
# Also drop GAME_ID
data$home_away <- as.factor(data$home_away)
home = rep(1, nrow(data))
home[as.numeric(data$home_away)==1] = 0
data$home <- home
data = subset(data, select = -c(s_FGM, s_FGA, b_FGM, b_FGA, home_away, GAME_ID))

data$team <- as.factor(data$team)
data$against <- as.factor(data$against)

# Move PTS_final to last column
data$PTS_final1 = data$PTS_final
data = subset(data, select = -PTS_final)
names(data)[names(data) == "PTS_final1"] <- "PTS_final"
```

```{r splitting training and testing data}
# Splitting training and testing data
set.seed(100)

# Dividing the dataset into training and testing datasets
testRows = sample(nrow(data),0.2*nrow(data))
test = data[testRows, ]
train = data[-testRows, ]
# Reindex dataframes
rownames(test) <- 1:nrow(test)
rownames(train) <- 1:nrow(train)

attach(train)
```

## Exploratory Data Analysis (EDA)
### Correlation Plot
```{r correlation plot}
cor <- cor(subset(train, select=-c(team, against)))
corrplot(cor)

```


### Histogram of Variables - Starter Stats
Red means heavy tail
```{r Histogram of variables (starter stats)}
# Histograms of variables - starters
par(mfrow = c(2, 3))
hist(s_minutes, main='s_minutes', col=1)
hist(s_3PM, main='s_3PM', col=2)
hist(s_3PA, main='s_3PA', col=1)
hist(s_FTM, main='s_FTM', col=2)
hist(s_OREB, main='s_OREB', col=2)
hist(s_DREB, main='s_DREB', col=1)
hist(s_AST, main='s_AST', col=1)
hist(s_STL, main='s_STL', col=2)
hist(s_BLK, main='s_BLK', col=2)
hist(s_TO, main='s_TO', col=1)
hist(s_FOUL, main='s_FOUL', col=1)
hist(s_2PA, main='b_2PA', col=1)

```


Histograms in red shows heavy tails, and the variables are s_3PM, s_FTM, s_OREB, s_STL, and s_BLK. 

### Histogram of Variables - Bench Stats
```{r Histogram of variables (bench stats)}
# Histograms of variables - bench
par(mfrow = c(2, 4))
hist(b_3PM, main='b_3PM', col=2)
hist(b_3PA, main='b_3PA', col=1)
hist(b_FTM, main='b_FTM', col=2)
hist(b_FTA, main='b_FTA', col=2)
hist(b_OREB, main='b_OREB', col=2)
hist(b_DREB, main='b_DREB', col=1)
hist(b_AST, main='b_AST', col=2)
hist(b_STL, main='b_STL', col=2)
hist(b_BLK, main='b_BLK', col=2)
hist(b_TO, main='b_TO', col=2)
hist(b_FOUL, main='b_FOUL', col=2)
hist(b_2PM, main='b_2PM', col=2)
hist(b_2PA, main='b_2PA', col=1)
```


Most stats for bench players have heavy tails (in red). The exceptions are 3PA, DREB, and 2PA

### Histogram of Variables - Total Score
```{r Histogram of response variable and score at halftime}
# Histogram of response variable and score at halftime
par(mfrow = c(1, 3))
hist(PTS_ht, main='PTS_ht', col=1)
hist(PTS_ht_opp, main='PTS_ht_opp', col=1)
hist(PTS_final, main='PTS_final', col=1)

```

All three are approximately normally distributed. 

### Scatterplot of predicting vs. response variables
```{r Scatterplots of predicting variables vs. PTS_final (starter)}
# Scatterplots of predicting variables vs. PTS_final (starter)
res_index = dim(train)[2]
pairs(train[,c(3:7, res_index)])
pairs(train[,c(8:12, res_index)])
pairs(train[,c(13:17, res_index)])
pairs(train[,c(18:22, res_index)])
pairs(train[,c(23:27, res_index)])
pairs(train[,c(28:31, res_index)])
```


## Variable Selection
We will utilize stepwise selection (backward and forward) with BIC as the parameter, mallow's cp, LASSO, and elastic net to select the "best" regression model. 

### Stepwise Regression
```{r Variable Selection 1 - stepwise regression with BIC}
# Stepwise regression with BIC
model_min = lm(PTS_final ~ PTS_ht, data=train) # Include PTS_ht as it's an obvious variable to include in the model
model_full = lm(PTS_final ~ . -team -against, data=train) # Exclude teams for simplicity

# Forward selection with BIC
model_forward = step(model_min, scope=list(lower=model_min,  upper=model_full), direction="forward", trace=F, k=log(nrow(train)))
summary(model_forward)

# Backward selection
model_backward = step(model_full, scope=list(lower=model_min,  upper=model_full), direction="backward", trace=F, k=log(nrow(train)))
summary(model_backward)

sprintf("model_forward includes: %s", paste(all.vars(formula(model_forward))[-1], collapse=', '))
sprintf("model_backward includes: %s", paste(all.vars(formula(model_backward))[-1], collapse=', '))

```

BIC is used for variable selection because the goal of the model is mostly for prediction. Interestingly, only 1 additional variable (PTS_ht_opp) is selected by model_backward, and 3 additional variables are selected by model_forward. 

### Mallow's cp
```{r Variable Selection 2 - Mallows Cp 1}
# Mallows Cp 
out = leaps(train[, c(3:31)], train[, res_index], method = "Cp", strictly.compatible = F, nbest = 1)
cbind(as.matrix(out$which), out$Cp)
best.model=which(out$Cp==min(out$Cp))
mellow_coef = cbind(as.matrix(out$which), out$Cp)[best.model,]
print(names(mellow_coef[mellow_coef==1]))
```


```{r Variable Selection 2 - Mallows Cp 2}
# Mallows Cp 
model_cp = lm(PTS_final ~ s_minutes+s_3PM+s_3PA+s_DREB+s_BLK+s_TO+b_FTM+b_OREB+
b_DREB+b_AST+b_STL+b_FOUL+PTS_ht+PTS_ht_opp+home, data=train)
summary(model_cp)
```
15 variables were selected using Mallow's Cp which is more than both forward (4) and backward (2) selection. 


### LASSO
```{r Variable Selection 3 - LASSO 1}
# LASSO - variable selection

# Set a seed for reproducibility
set.seed(1)

# Set predictors and response to correct format
x.train <- model.matrix(PTS_final ~ ., train[,c(3:res_index)])
y.train <- train$PTS_final

# Use cross validation to find optimal lambda
cv.lasso <- cv.glmnet(x.train, y.train, alpha = 1)

# Train Lasso and display coefficients with optimal lambda
lasso.model <- glmnet(x.train, y.train, alpha = 1)
coef(lasso.model, cv.lasso$lambda.min)
```
With LASSO, it removed s_FTM, s_FOUL, b_FTA, and b_2PA. A new model will be fit below to find coefficients. Furthermore, since multicollinearity may be a concern with our data, elastic net should also be attempted to fit model. 

```{r Variable Selection 3 - LASSO 2}
# LASSO - fit model
model_lasso = lm(PTS_final ~ . -s_FTM -s_FOUL -b_FTA -b_2PA, train[,c(3:res_index)])
summary(model_lasso)
```


### Elastic Net
```{r Variable Selection 4 - Elastic Net 1}
# Elastic Net - Variable Selection

# Set a seed for reproducibility
set.seed(1)

# Set predictors and response to correct format
x.train <- model.matrix(PTS_final ~ ., train[,c(3:res_index)])
y.train <- train$PTS_final

# Use cross validation to find optimal lambda
cv.elasticnet <- cv.glmnet(x.train, y.train, alpha = 0.5)

# Train Lasso and display coefficients with optimal lambda
elasticnet.model <- glmnet(x.train, y.train, alpha = 0.5)
coef(elasticnet.model, cv.elasticnet$lambda.min)
```
With elastic net variable selection, same group of variables were removed as LASSO. 

```{r Variable Selection 4 - Elastic Net 2}
# Elastic net - fit model
model_net = lm(PTS_final ~ . -s_FTM -s_FOUL -b_FTA -b_2PA, train[,c(3:res_index)])
summary(model_net)
```


### Summary of Model Selection
```{r Variable Selection - Final}
# Compare all selected models based on AIC, BIC, and adjusted R^2
n = nrow(train)
model_full_stats = list("Adj R2" = summary(model_full)$adj.r.squared, "AIC" = AIC(model_full, k=2), "BIC" = AIC(model_full, k=log(n)), "DF"=summary(model_full)$df[3])
model_forward_stats = list("Adj R2" = summary(model_forward)$adj.r.squared, "AIC" = AIC(model_forward, k=2), "BIC" = AIC(model_forward, k=log(n)), "DF"=summary(model_forward)$df[3])
model_backward_stats = list("Adj R2" = summary(model_backward)$adj.r.squared, "AIC" = AIC(model_backward, k=2), "BIC" = AIC(model_backward, k=log(n)), "DF"=summary(model_backward)$df[3])
model_cp_stats = list("Adj R2" = summary(model_cp)$adj.r.squared, "AIC" = AIC(model_cp, k=2), "BIC" = AIC(model_cp, k=log(n)), "DF"=summary(model_cp)$df[3])
model_lasso_stats = list("Adj R2" = summary(model_lasso)$adj.r.squared, "AIC" = AIC(model_lasso, k=2), "BIC" = AIC(model_lasso, k=log(n)), "DF"=summary(model_lasso)$df[3])
model_net_stats = list("Adj R2" = summary(model_net)$adj.r.squared, "AIC" = AIC(model_net, k=2), "BIC" = AIC(model_net, k=log(n)), "DF"=summary(model_net)$df[3])
model_compare = rbind(model_full_stats, model_forward_stats, model_backward_stats, model_cp_stats, model_lasso_stats, model_net_stats)
model_compare

```

All models have similar adjusted R^2, AIC, and BIC. For simplicity sake, model_cp is probably the best considering relatively high adjusted R^2, lowest AIC, relatively low BIC, and moderate number of variables. 

## Assess Model Assumption

```{r Model Summary}
# Display model summary
summary(model_cp)

```

### Residual Analysis - Predicting Variables

```{r Residual Analysis - Predicting Variables}
# Residual Analysis
vars = all.vars(formula(model_cp))[-1]

resids = stdres(model_cp)
par(mfrow=(c(2,3)))

for (v in vars) {
  for (i in 1:dim(train)[-1]){
    if (colnames(train[i]) == v){
      plot(train[,i], resids, xlab=v, ylab='Residuals')
      abline(0,0, col='red')
    }
  }
}

```

Variables with obvious constant variance violence: b_FTM, b_OREB, b_STL, home. 

Since home is a binary variable, it won't benefit from transformation. We can try log transformation on other 3 variables. 

### Residual Analysis - Response Variable

```{r Residual Analysis - Response Variable}
par(mfrow=(c(2,2)))
cook = cooks.distance(model_cp)
hist(resids, xlab="Residuals", main= "Histogram of Residuals")
qqnorm(resids)
qqline(resids)
plot(cook,type="h",lwd=3, ylab = "Cookâ€™s Distance")
abline(h=4/nrow(train), col='red')
```

The residuals follows normal distribution without a heavy tail, and a transformation of the response variable is not needed. 

Cook's Distance may have shown significant number of outliers when using 4/n as the threshold; however, it's not a good rule of thumb to follow when the number of samples is large. From the plot, we can consider data whose Cook's Distance is greater than 0.005 to be outliers and build model without them and see if the effects are significant. 

### Outlier Analysis
```{r Refit Model without Outliers}
# Refit model without outliers
outliers = cook[cook>0.005]
outliers_index = as.integer(labels(outliers))


model_no_outliers = lm(PTS_final ~ s_minutes+s_3PM+s_3PA+s_DREB+s_BLK+s_TO+b_FTM+b_OREB+
b_DREB+b_AST+b_STL+b_FOUL+PTS_ht+PTS_ht_opp+home, data=train[-c(outliers_index),])

# Compare model coefficients to evaluate the effect of outliers
coef(model_no_outliers)
coef(model_cp)
```

The coefficients are very similar between the 2 models with and without outliers; therefore, we can keep the outliers in the model and we do not need to create separate models. 

### Multicollinearity Analysis

```{r Model evaluation - multicollinearity}
# Evaluate multicollinearity
sprintf("VIF threshold is: %i", max(1/summary(model_cp)$r.squared, 10))
vif(model_cp)
```

All VIF values are less than 3, and multicollinearity is not a concern. 

### Perform Transformation

```{r Perform Transformation}
# Transformation of model_cp (b_FTM, b_OREB, b_STL)
model_trans = lm(formula = PTS_final ~ s_minutes + s_3PM + s_3PA + s_DREB + 
    s_BLK + s_TO + log(b_FTM+1) + log(b_OREB+1) + b_DREB + b_AST + log(b_STL+1) + 
    b_FOUL + PTS_ht + PTS_ht_opp + home, data = train)
summary(model_trans)
```

### Residual Analysis on Transformed Model

```{r }
# Residual Analysis (with log transformed b_FTM, b_OREB, b_STL)
vars = all.vars(formula(model_trans))[-1]

resids = stdres(model_trans)
par(mfrow=(c(2,3)))

for (v in vars) {
  for (i in 1:dim(train)[-1]){
    if (colnames(train[i]) == v){
      if (colnames(train[i]) %in% list('b_FTM', 'b_OREB', 'b_STL')){
        plot(log(train[,i]+1), resids, xlab=v, ylab='Residuals')
        abline(0,0, col='red')
      } else {
      plot(train[,i], resids, xlab=v, ylab='Residuals')
      abline(0,0, col='red')
      }
    }
  }
}

```


After transformation, the variances of b_FTM, b_OREB, and b_STL appears to be more of a constant although it's still not perfect. 

### Model Evaluation
#### MSPE & PE

```{r Regression Model Evaluation - MSPE & PE}
# Use Test data to evaluate its ability to predict final score
prediction = predict(model_trans, test)
hist(prediction-test$PTS_final)
y_bar = mean(test$PTS_final)
MSPE = sum((prediction - test$PTS_final) ** 2) / nrow(test)
PE = sum((prediction - test$PTS_final)**2) / sum((test$PTS_final - y_bar)**2)
sprintf("The mean squared prediction error (MSPE) is %f", MSPE)
sprintf("The Precision error is %f", PE)

```

#### Prediction Interval
```{r Regression Model Evaluation - Prediction Interval}
# Evaluate the model using prediction interval
confidence = c()
percent = c()
for (i in seq(0.5, 0.99, 0.02)){
  pred_int = predict(model_trans, test, interval = 'prediction', level=i)
  p = sum(test$PTS_final < pred_int[,3] & test$PTS_final > pred_int[,2]) / nrow(test) * 100
  confidence = c(confidence, i)
  percent = c(percent, p)
}

plot(confidence, percent/confidence)
```

Above is the plot for confidence of prediction interval (x-axis) vs. percentage of result within prediction interval divided by the prediction interval. By definition, the ratio should be very close or equal to 1. We can see in the plot that at around 75% confidence interval we have the highest ratio. Therefore, we can use 75% confidence interval to estimate the final score of each team using box score at half time with greatest possibility of success considering the width of prediction interval. 


## Conclusion and Future Work
The developed model hopefully has more than 80% accuracy in predicting the final score using box score of both team at halftime. The results can be used to place bet on the score of each team, combined score, or score differential at halftime. 

Machine learning can be utilized to improve the model and hopefully narrow the prediction interval to achieve better results. 
```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```